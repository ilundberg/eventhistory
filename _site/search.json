[
  {
    "objectID": "bernoulli.html",
    "href": "bernoulli.html",
    "title": "Bernoulli",
    "section": "",
    "text": "Let \\(y_1,\\dots,y_n \\stackrel{\\text{iid}}{\\sim} \\text{Bernoulli}(\\pi)\\) be \\(n\\) independent flips of a weighted coin with probability \\(\\pi\\) of heads. In basic statistics, you already learned a common estimator \\(\\hat\\pi = \\frac{1}{n}\\sum_i y_i\\). You also learned that this estimator is asymptotically Normally distributed, with estimated variance \\(\\hat{\\text{V}}(\\hat\\pi) = \\frac{\\hat\\pi(1 - \\hat\\pi)}{n}\\).\nThis page will derive those same properties via Maximum Likelihood Estimation."
  },
  {
    "objectID": "bernoulli.html#math",
    "href": "bernoulli.html#math",
    "title": "Bernoulli",
    "section": "Math",
    "text": "Math\nThe likelihood function is the probability of the data given the parameter \\(\\pi\\).\n\\[\\begin{aligned}\n        L(\\pi\\mid \\vec{y}) &= \\text{P}(\\vec{Y} = \\vec{y}\\mid \\pi) &\\text{by definition of likelihood} \\\\\n        &= \\prod_{i=1}^n \\text{P}(Y_i = y_i) &\\text{by independence} \\\\\n        &= \\prod_{i=1}^n \\pi^{y_i}(1-\\pi)^{1-y_i} &\\text{by Bernoulli probability mass function} \\\\\n        &= \\pi^{\\sum_i y_i}(1 - \\pi)^{n - \\sum_i y_i} &\\text{by rules of exponents}\n\\end{aligned}\\]\nThe log likelihood is often easier to optimize. Taking the log,\n\\[\\begin{aligned}\n\\ell(\\pi\\mid\\vec{y}) &= \\log(L(\\pi\\mid\\vec{y})) \\\\\n        &= \\log\\left(\\pi^{\\sum_i y_i}(1 - \\pi)^{n - \\sum_i y_i}\\right) \\\\\n        &= (\\sum_i y_i)\\log(\\pi) + (n - \\sum_i y_i)\\log(1 - \\pi) &\\text{by rules of logs}\n\\end{aligned}\\]\nWe want to find the estimate \\(\\hat\\pi\\) that maximizes \\(L(\\pi\\mid\\vec{y})\\). This is at a place where the derivative (slope) is zero. To find it, take the derivative.\n\\[\n\\begin{aligned}\n        \\frac{\\partial}{\\partial \\pi} \\ell(\\pi)\n        &= \\frac{\\sum_i y_i}{\\pi} - \\frac{n - \\sum_i y_i}{1 - \\pi}\n\\end{aligned}\n\\] Set equal to zero and solve. Let \\(\\tilde\\pi\\) be a point where the derivative equals zero.\n\\[\n\\begin{aligned}\n        0 &= \\frac{\\partial}{\\partial \\pi} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\tilde{\\pi}} \\\\\n        0 &= \\frac{\\sum_i y_i}{\\tilde\\pi} - \\frac{n - \\sum_i y_i}{1 - \\tilde\\pi} \\\\\n        \\tilde\\pi &= \\frac{1}{n} \\sum_i y_i\n\\end{aligned}\n\\]\nThe likelihood function is flat at the point where \\(\\tilde\\pi\\) equals the sample mean of \\(Y\\). Next, check the second derivative to see if this is a maximum.\n\\[\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y}) &= -\\frac{\\sum_i y_i}{\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\pi)^2} \\\\\n&= -\\frac{\\text{positive}}{\\text{positive}} - \\frac{\\text{positive}}{\\text{positive}} \\\\\n        & &lt; 0\n\\end{aligned}\\]\nSo it is a maximum! The second derivative of the log likelihood at the value that maximizes the function is called the Hessian. It is useful for deriving the standard error. Next we derive the Hessian. \\[\\begin{aligned}\nH(\\pi) &= \\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\hat\\pi_\\text{MLE}} &\\text{definition of Hessian} \\\\\n&= -\\frac{\\sum_i y_i}{\\hat\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\hat\\pi)^2} \\\\\n&= -\\frac{n\\hat\\pi}{\\hat\\pi^2} - \\frac{n - n\\hat\\pi}{(1 - \\hat\\pi)^2} &\\text{by replacing }\\sum_i y_i\\text{ with }n\\hat\\pi \\\\\n&= -\\frac{n}{\\hat\\pi}-\\frac{n}{1 - \\hat\\pi} \\\\\n&= -\\frac{n}{\\hat\\pi(1 - \\hat\\pi)}\n\\end{aligned}\\]\nFrom the statistical theory of Maximum Likelihood Estimation, the asymptotic variance of an MLE estimate is the negative inverse Hessian (also called the Fisher Information).\n\\[\\begin{aligned}\nV(\\hat\\pi) &= -\\left[H(\\hat\\pi)\\right]^{-1} \\\\\n&= \\frac{\\hat\\pi(1 - \\hat\\pi)}{n}\n\\end{aligned}\\]\nwhich is a formulat that may be familiar! Because of the Central Limit Theorem, \\(\\hat\\pi\\) is asymptotically Normally distributed with this mean and variance. Thus we can construct a 95% confidence interval by a Normal approximation.\n\\[\\begin{aligned}\n\\hat\\pi_\\text{Lower} &= \\hat\\pi - \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}} \\\\\n\\hat\\pi_\\text{Upper} &= \\hat\\pi + \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}}\n\\end{aligned}\\]\nRecap: The exercise used the theory of Maximum Likelihood Estimation to produce familiar formulas:\n\nestimator of a proportion is the sample mean\nvariance of that estimated proportion\nconfidence intervals by asymptotic Normality"
  },
  {
    "objectID": "bernoulli.html#numerical-optimization",
    "href": "bernoulli.html#numerical-optimization",
    "title": "Bernoulli",
    "section": "Numerical optimization",
    "text": "Numerical optimization\nHere we will numerically optimize the log likelihood.\n\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\n\nWithout predictors\nWe first illustrate for the marginal case: estimating a constant \\(\\pi\\) that applies to all cases with no predictors. We know from math that the MLE is the sample mean, \\(\\hat\\pi_\\text{MLE} = \\frac{1}{n}\\sum_i y_i\\). But what if we only knew the likelihood function? We could optimize numerically, as we illustrate with simulated data with \\(\\pi = 0.4\\).\n\ny &lt;- rbinom(n = 1000, size = 1, prob = .4)\n\nIn the marginal case, we saw above that the log likelihood is \\[\n\\ell(\\pi\\mid\\vec{y}) = \\sum_i \\left(y_i\\log(\\pi) + (1 - y_i)\\log(1 - \\pi)\\right)\n\\]\nWe can code this in a function.\n\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\nThen we can use optimize to find the maximum of the function.\n\noptimize.out &lt;- optimize(\n  log_likelihood,  # function to optimize\n  lower = 0,       # lower limit of pi candidates\n  upper = 1,       # upper limit of pi candidates\n  maximum = TRUE,  # search for maximum\n  tol = .01,       # get within tol of truth\n  y = y            # other argument to log_likelihood\n)\n\nOur estimate is stored in the maximum list item of the resulting object.\n\noptimize.out$maximum\n\n[1] 0.4163974\n\n\nWe can note that this is approximately equal to the analytical result that we know is the maximum: the sample mean.\n\nmean(y)\n\n[1] 0.415\n\n\n\n\nWith predictors\nNumerical optimization is often most helpful for generalized linear models that include predictors. For practice, the code below generates data with numeric predictors x1 and x2 and binary outcome y.\n\nsimulated &lt;- tibble(id = 1:100) |&gt;\n  mutate(\n    x1 = rnorm(n()),\n    x2 = rnorm(n()),\n    pi = plogis(x1 + x2),\n    y = rbinom(n(), 1, pi)\n  )\n\nThe optim function in R carries out numerical optimization. To use it, we first need to write down a function for our log likelihood.\n\nlog_likelihood &lt;- function(parameters, data, formula) {\n  # Create the beta vector given the parameters\n  beta &lt;- parameters\n  X &lt;- model.matrix(formula, data = data)\n  y &lt;- data$y\n  log_likelihood &lt;- sum(\n    y * log(plogis(X %*% beta)) + \n      (1 - y) * log(1 - plogis(X %*% beta))\n  )\n  return(log_likelihood)\n}\n\nWe optimize this with a call to the optim function.\n\noptim.out &lt;- optim(\n  par = c(0,0,0), # initial values of the parameters\n  fn = log_likelihood,\n  control = list(fnscale = -1),\n  hessian = TRUE,   # multiplies by -1 to find maximum instead of minimum\n  # Other arguments to be passed to log_likelihood\n  data = simulated,\n  formula = formula(y ~ x1 + x2)\n)\n\nWe can extract the coefficient estimates.\n\nbeta_hat &lt;- optim.out$par |&gt; print()\n\n[1] 0.01887572 0.83477290 0.83743034\n\n\nWe can extract the Hessian.\n\nhessian &lt;- optim.out$hessian\n\nWe can solve for the variance-covariance matrix \\(\\hat{\\text{V}}(\\hat{\\vec\\beta})\\): the negative inverse Hessian. In R, the solve function finds the inverse of a matrix.\n\nbeta_hat_vcov &lt;- -solve(hessian)\n\nThis can give us our coefficients and standard errors (the square root of the diagonal of the variance covariance matrix).\n\ntibble(\n  variable = c(\"Intercept\",\"x1\",\"x2\"),\n  beta = beta_hat,\n  se = sqrt(diag(beta_hat_vcov))\n)\n\n# A tibble: 3 × 3\n  variable    beta    se\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Intercept 0.0189 0.230\n2 x1        0.835  0.244\n3 x2        0.837  0.253\n\n\nWe can compare that to the output from logistic regression.\n\ncanned_fit &lt;- glm(y ~ x1 + x2, data = simulated, family = binomial)\nsummary(canned_fit)\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial, data = simulated)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.01887    0.22988   0.082 0.934562    \nx1           0.83507    0.24358   3.428 0.000607 ***\nx2           0.83748    0.25280   3.313 0.000924 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 137.99  on 99  degrees of freedom\nResidual deviance: 114.73  on 97  degrees of freedom\nAIC: 120.73\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n\nSimulating predicted probabilities\nWe can simulate predicted probabilities. Suppose we are interested in the prediction at \\((X_1,X_2) = (1,1)\\). Define these data to predict,\n\nto_predict &lt;- tibble(x1 = 0, x2 = 1)\n\ncreate an \\(X\\) matrix at those values\n\nX_new &lt;- model.matrix(~ x1 + x2, data = to_predict)\n\nand make predictions from that \\(X\\) matrix.\n\npi_hat &lt;- plogis(X_new %*% beta_hat) |&gt; print()\n\n       [,1]\n1 0.7018883\n\n\nTo get a standard error on the predictions, we can use simulation. We know that \\(\\hat{\\vec\\beta}\\) is asymptotically multivariate normal. We can simulate many coefficient vectors \\(\\vec\\beta^*\\) from that distribution,\n\nbeta_star &lt;- mvtnorm::rmvnorm(n = 1000, mean = beta_hat, sigma = beta_hat_vcov)\n\nand we can generate a predicted probability from each simulated value.\n\npi_star &lt;- plogis(X_new %*% t(beta_star))\n\nThe standard error is the standard deviation of these simulated draws.\n\nse_pi_hat &lt;- sd(pi_star)\n\nWe can likewise predict with the canned version of the model,\n\ncanned_prediction &lt;- predict(canned_fit, type = \"response\", newdata = to_predict, se = T)\n\nand compare to see that they are approximately the same.\n\ntibble(method = c(\"diy\",\"canned\")) |&gt;\n  mutate(\n    pi_hat = c(pi_hat, canned_prediction$fit),\n    se = c(se_pi_hat, canned_prediction$se.fit)\n  )\n\n# A tibble: 2 × 3\n  method pi_hat     se\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 diy     0.702 0.0662\n2 canned  0.702 0.0683"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "exponential.html#canned-code",
    "href": "exponential.html#canned-code",
    "title": "Marginal Exponential Model",
    "section": "Canned Code",
    "text": "Canned Code"
  },
  {
    "objectID": "exponential.html#under-the-hood",
    "href": "exponential.html#under-the-hood",
    "title": "Marginal Exponential Model",
    "section": "Under the Hood",
    "text": "Under the Hood"
  },
  {
    "objectID": "bernoulli.html#derivations-in-math",
    "href": "bernoulli.html#derivations-in-math",
    "title": "Bernoulli",
    "section": "Derivations in math",
    "text": "Derivations in math\n\nMarginal setting\nThe likelihood function is the probability of the data given the parameter \\(\\pi\\).\n\\[\\begin{aligned}\n        L(\\pi\\mid \\vec{y}) &= \\text{P}(\\vec{Y} = \\vec{y}\\mid \\pi) &\\text{by definition of likelihood} \\\\\n        &= \\prod_{i=1}^n \\text{P}(Y_i = y_i) &\\text{by independence} \\\\\n        &= \\prod_{i=1}^n \\pi^{y_i}(1-\\pi)^{1-y_i} &\\text{by Bernoulli probability mass function}\n\\end{aligned}\\]\nThe log likelihood is often easier to optimize. Taking the log,\n\\[\\begin{aligned}\n\\ell(\\pi\\mid\\vec{y}) &= \\log(L(\\pi\\mid\\vec{y})) \\\\\n        &= \\log\\left(\\prod_i \\pi^{y_i}(1-\\pi)^{1-y_i}\\right) \\\\\n        &= \\sum_i\\log\\left(\\pi^{y_i}(1-\\pi)^{1-y_i}\\right) &\\text{since }\\log(AB) = \\log(A) + \\log(B) \\\\\n        &= \\sum_i\\left(\\log(\\pi^{y_i}) + \\log((1-\\pi)^{1-y_i})\\right) &\\text{since }\\log(AB) = \\log(A) + \\log(B) \\\\\n        &= \\sum_i \\left(y_i\\log(\\pi) + (1 - y_i)\\log(1 - \\pi)\\right) &\\text{since}\\log(A^B)=B\\log(A)\n\\end{aligned}\\]\nWe want to find the estimate \\(\\hat\\pi\\) that maximizes \\(L(\\pi\\mid\\vec{y})\\). This is at a place where the derivative (slope) is zero. To find it, take the derivative.\n\\[\n\\begin{aligned}\n        \\frac{\\partial}{\\partial \\pi} \\ell(\\pi)\n        &= \\frac{\\sum_i y_i}{\\pi} - \\frac{n - \\sum_i y_i}{1 - \\pi}\n\\end{aligned}\n\\] Set equal to zero and solve. Let \\(\\tilde\\pi\\) be a point where the derivative equals zero.\n\\[\n\\begin{aligned}\n        0 &= \\frac{\\partial}{\\partial \\pi} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\tilde{\\pi}} \\\\\n        0 &= \\frac{\\sum_i y_i}{\\tilde\\pi} - \\frac{n - \\sum_i y_i}{1 - \\tilde\\pi} \\\\\n        \\tilde\\pi &= \\frac{1}{n} \\sum_i y_i\n\\end{aligned}\n\\]\nThe likelihood function is flat at the point where \\(\\tilde\\pi\\) equals the sample mean of \\(Y\\). Next, check the second derivative to see if this is a maximum.\n\\[\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y}) &= -\\frac{\\sum_i y_i}{\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\pi)^2} \\\\\n&= -\\frac{\\text{positive}}{\\text{positive}} - \\frac{\\text{positive}}{\\text{positive}} \\\\\n        & &lt; 0\n\\end{aligned}\\]\nSo it is a maximum! The second derivative of the log likelihood at the value that maximizes the function is called the Hessian. It is useful for deriving the standard error. Next we derive the Hessian. \\[\\begin{aligned}\nH(\\pi) &= \\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\hat\\pi_\\text{MLE}} &\\text{definition of Hessian} \\\\\n&= -\\frac{\\sum_i y_i}{\\hat\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\hat\\pi)^2} \\\\\n&= -\\frac{n\\hat\\pi}{\\hat\\pi^2} - \\frac{n - n\\hat\\pi}{(1 - \\hat\\pi)^2} &\\text{by replacing }\\sum_i y_i\\text{ with }n\\hat\\pi \\\\\n&= -\\frac{n}{\\hat\\pi}-\\frac{n}{1 - \\hat\\pi} \\\\\n&= -\\frac{n}{\\hat\\pi(1 - \\hat\\pi)}\n\\end{aligned}\\]\nFrom the statistical theory of Maximum Likelihood Estimation, the asymptotic variance of an MLE estimate is the negative inverse Hessian (also called the Fisher Information).\n\\[\\begin{aligned}\nV(\\hat\\pi) &= -\\left[H(\\hat\\pi)\\right]^{-1} \\\\\n&= \\frac{\\hat\\pi(1 - \\hat\\pi)}{n}\n\\end{aligned}\\]\nwhich is a formulat that may be familiar! Because of the Central Limit Theorem, \\(\\hat\\pi\\) is asymptotically Normally distributed with this mean and variance. Thus we can construct a 95% confidence interval by a Normal approximation.\n\\[\\begin{aligned}\n\\hat\\pi_\\text{Lower} &= \\hat\\pi - \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}} \\\\\n\\hat\\pi_\\text{Upper} &= \\hat\\pi + \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}}\n\\end{aligned}\\]\nRecap: The exercise used the theory of Maximum Likelihood Estimation to produce familiar formulas:\n\nestimator of a proportion is the sample mean\nvariance of that estimated proportion\nconfidence intervals by asymptotic Normality\n\n\n\nGeneralized Linear Model setting\nWhat if every person \\(i\\) flips their own coin with probability \\(\\pi_i\\), as in logistic regression? Suppose for simplicity that each person has covariates \\(\\vec{x}_i\\) and the probability follows the functional form of logistic regression.\n\\[\n\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\vec{X}_i\\vec\\beta\n\\]\nWe will refer to this function as the logit function and will use it and its inverse.\n\\[\n\\begin{align}\n&\\text{Logit function:} & \\text{logit}(x) &= \\log\\left(\\frac{x}{1 - x}\\right) \\\\\n&\\text{Inverse logit function:} & \\text{logit}^{-1}(x) &= \\frac{1}{1 + e^{-x}} \\\\\n\\end{align}\n\\]\nThese functions are useful because in logistic regression \\(\\text{logit}(\\pi_i) = \\vec{X}_i\\vec\\beta\\) and \\(\\text{logit}^{-1}(\\vec{X}_i\\vec\\beta) = \\pi\\). This is helpful because:\n\nany likelihood in terms of \\(\\pi_i\\) is equivalently a likelihood in terms of \\(\\vec\\beta\\)\nthese functions are available in R: qlogis is logit and plogis is the inverse logit\n\nMaximum likelihood for logistic regression becomes the task: what parameter vector \\(\\vec\\beta\\) produces a probability vector \\(\\vec\\pi\\) under which the data \\(\\vec{y}\\) are most likely?\nAppealing to the marginal case above, the log likelihood is\n\\[\\begin{aligned}\n\\ell(\\vec\\pi\\mid\\vec{y}) &= \\sum_i\\left(y_i\\log\\pi_i + (1 - y_i)\\log(1 - \\pi_i)\\right)\n\\end{aligned}\\]\nUsing the inverse logit function, we express this in terms of \\(\\vec\\beta\\). \\[\\begin{aligned}\n\\ell(\\vec\\beta\\mid\\vec{y})\n&= \\sum_i\\left(y_i\\log(\\text{logit}^{-1}(\\vec{X}_i\\vec\\beta)) + (1 - y_i)\\log(1 - \\text{logit}^{-1}(\\vec{X}_i\\vec\\beta))\\right)\n\\end{aligned}\\] Rather than solve this analytically, we will work with it by numerically optimizing the log likelihood."
  },
  {
    "objectID": "exponential.html",
    "href": "exponential.html",
    "title": "Exponential",
    "section": "",
    "text": "An Exponential survival model is a Generalized Linear Model just like logistic regression (previous page). We will estimate this model by writing down the log likelihood and carrying out numerical optimization with optim. As with the previous model, we will recover estimates that match those produced by canned functions.\nAs a reminder, the Exponential(1) distribution looks like this:"
  },
  {
    "objectID": "exponential.html#without-predictors",
    "href": "exponential.html#without-predictors",
    "title": "Exponential",
    "section": "Without predictors",
    "text": "Without predictors\nAssume a data generating process of \\(n\\) independent observations.\n\n\\(t_1,\\dots,t_n\\sim\\text{Exponential}(\\lambda)\\) are event times\n\\(\\tilde{t}_1,\\dots,\\tilde{t}_n\\) are observation times (either events or censoring)\n\\(c_1,\\dots c_n\\) indicate whether an observation is censored (\\(c_i = 1\\)) or the event occurs (\\(c_i = 0\\))\n\nThe code below will simulate data with \\(\\lambda = 1\\) and censoring at \\(t = 3\\).\n\nsimulated &lt;- tibble(id = 1:1000) |&gt;\n  mutate(\n    # Exponential draws\n    t = rexp(n()),\n    # Trial cuts off at time 3\n    c = t &gt; 3,\n    # Observed y is truncated at 3\n    t_tilde = ifelse(t &gt; 3, 3, t)\n  )"
  },
  {
    "objectID": "exponential.html#with-predictors",
    "href": "exponential.html#with-predictors",
    "title": "Exponential",
    "section": "With predictors",
    "text": "With predictors\nNow consider the setting where \\(\\lambda_i\\) varies across units \\(i\\) according to a Generalized Linear model, \\[\n\\begin{aligned}\ny_i&\\sim\\text{Exponential}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\vec{X}_i\\vec\\beta\n\\end{aligned}\n\\]\nThe code below generates data to illustrate with two predictors, \\(X_1\\) and \\(X_2\\).\n\nsimulated &lt;- tibble(id = 1:1000) |&gt;\n  mutate(\n    x1 = rnorm(n()),\n    x2 = rnorm(n()),\n    lambda = exp(-1 + .5 * x1 + .5 * x2),\n    t = rexp(n(), rate = lambda),\n    # Create censoring at time 3\n    c = t &gt; 3,\n    t_tilde = ifelse(t &gt; 3, 3, t)\n  ) |&gt;\n  select(x1, x2, c, t_tilde)"
  },
  {
    "objectID": "exponential.html#marginal-likelihood-in-math",
    "href": "exponential.html#marginal-likelihood-in-math",
    "title": "Exponential",
    "section": "Marginal likelihood in math",
    "text": "Marginal likelihood in math\nLet \\(f(t,\\lambda)\\) be the PDF of the exponential distribution. Let \\(F(t,\\lambda)\\) be the CDF. The likelihood is the probability of observing the data if the parameter takes the value \\(\\lambda\\). The observed data either tells us:\n\nan event occurred at time \\(t\\) (uncensored)\n\noccurs with probability density \\(f(t\\mid\\lambda)\\)\n\nan event occurred at time greater than \\(t\\) (censoring)\n\noccurs with probability \\(1 - F(t\\mid\\lambda)\\)\n\n\nTranslating to math, the likelihood for a given observation is \\[\n\\underbrace{\\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}}_{\\text{PDF at }\\tilde{t}_i\\text{ if uncensored}}\\quad\\times\\quad \\underbrace{\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}}_{\\text{Survival past }\\tilde{t}_i\\text{ if censored}}\n\\]\nWe can put these together into a likelihood function for the vector of independent observations, \\[\nL(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\prod_i \\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}\n\\] and take the log to get the log likelihood. \\[\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda)]\\right)\n\\]"
  },
  {
    "objectID": "exponential.html#marginal-likelihood-in-code-the-likelihood",
    "href": "exponential.html#marginal-likelihood-in-code-the-likelihood",
    "title": "Exponential",
    "section": "Marginal likelihood in code the likelihood",
    "text": "Marginal likelihood in code the likelihood\nWrite the log likelihood as a function in R.\n\nlog_likelihood &lt;- function(log_lambda, data) {\n  data |&gt;\n    mutate(\n      likelihood_i = case_when(\n        c == 0 ~ dexp(t_tilde, rate = exp(log_lambda)),\n        c == 1 ~ pexp(t_tilde, rate = exp(log_lambda), lower.tail = FALSE)\n      )\n    ) |&gt;\n    summarize(\n      log_likelihood = sum(log(likelihood_i))\n    ) |&gt;\n    pull(log_likelihood)\n}"
  },
  {
    "objectID": "exponential.html#one-parameter-optimization",
    "href": "exponential.html#one-parameter-optimization",
    "title": "Exponential",
    "section": "One-parameter optimization",
    "text": "One-parameter optimization\nUsing optim, we can numerically find the value \\(\\hat\\lambda\\) that maximizes the log likelihood function.\n\noptimize.out &lt;- optimize(\n  log_likelihood,  # function to optimize\n  lower = -1,      # lower limit of log_lambda candidates\n  upper = 1,       # upper limit of log_lambda candidates\n  maximum = TRUE,  # search for maximum\n  tol = .01,       # get within tol of truth\n  data = simulated # other argument to log_likelihood\n)\n\nRemember that we set our parameter to be the log of the rate \\(\\lambda\\). Thus, we need to exponentiate the estimated parameter.\n\nlog_lambda_hat &lt;- optimize.out$maximum\nlambda_hat &lt;- exp(log_lambda_hat) |&gt; print()\n\n[1] 1.014719"
  },
  {
    "objectID": "exponential.html#conditional-likelihood-in-math",
    "href": "exponential.html#conditional-likelihood-in-math",
    "title": "Exponential",
    "section": "Conditional likelihood in math",
    "text": "Conditional likelihood in math\nThe likelihood has not changed much from the case without predictors. The \\(\\lambda\\) terms become \\(\\lambda_i\\), \\[\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\vec\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda_i)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda_i)]\\right)\n\\]\nand when coding this we will use the assumption that \\(\\lambda_i = \\text{exp}(\\vec{X}_i\\vec\\beta)\\)."
  },
  {
    "objectID": "exponential.html#conditional-likelihood-in-code",
    "href": "exponential.html#conditional-likelihood-in-code",
    "title": "Exponential",
    "section": "Conditional likelihood in code",
    "text": "Conditional likelihood in code\nThe log likelihood takes parameters and data and returns a likelihood value. When coding this, it is helpful that the dexp and pexp functions are the PDF and CDF of the Exponential distribution with rate parameter equal to \\(\\lambda\\).\n\nlog_likelihood &lt;- function(parameters, data, formula) {\n  \n  # Parameters are coefficients\n  beta &lt;- parameters\n  \n  # Get the X matrix\n  X &lt;- model.matrix(formula, data = data)\n  \n  # Calculate lambda values at that parameter vector\n  lambda &lt;- exp(X %*% beta)\n  \n  # Calculate the likelihood\n  data |&gt;\n    # Create a column with the lambda values for each case\n    mutate(lambda = lambda) |&gt;\n    summarize(\n      # Use the formula from above, translated to code\n      log_likelihood = sum(\n        (1 - c) * log(dexp(t_tilde, rate = lambda)) + \n          c * log(1 - pexp(t_tilde, rate = lambda))\n      )\n    ) |&gt;\n    # Pull the log likelihood value to return\n    pull(log_likelihood)\n}"
  },
  {
    "objectID": "exponential.html#vector-parameter-optimization",
    "href": "exponential.html#vector-parameter-optimization",
    "title": "Exponential",
    "section": "Vector-parameter optimization",
    "text": "Vector-parameter optimization\nNow optimize with a call to optim.\n\noptim.out &lt;- optim(\n  par = c(0,0,0),                # initial parameter values\n  fn = log_likelihood,           # function to optimize\n  control = list(fnscale = -1),  # find max instead of min\n  hessian = TRUE,                # also return the Hessian\n  data = simulated,              # passed to log_likelihood\n  formula = formula(t_tilde ~ x1 + x2) # passed to log_likelihood\n)\n\nWe can extract the coefficient estimates.\n\nbeta_hat &lt;- optim.out$par |&gt; print()\n\n[1] -1.0030177  0.4808066  0.4421136\n\n\nWe can extract the Hessian.\n\nhessian &lt;- optim.out$hessian\n\nWe can solve for the variance-covariance matrix \\(\\hat{\\text{V}}(\\hat{\\vec\\beta})\\): the negative inverse Hessian. In R, the solve function finds the inverse of a matrix.\n\nbeta_hat_vcov &lt;- -solve(hessian)\n\nThis can give us our coefficients and standard errors (the square root of the diagonal of the variance covariance matrix).\n\ntibble(\n  variable = c(\"Intercept\",\"x1\",\"x2\"),\n  beta = beta_hat,\n  se = sqrt(diag(beta_hat_vcov))\n)\n\n# A tibble: 3 × 3\n  variable    beta     se\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Intercept -1.00  0.0411\n2 x1         0.481 0.0404\n3 x2         0.442 0.0415\n\n\nWe can compare that to the output from logistic regression.\n\nlibrary(survival)\ncanned_fit &lt;- survreg(\n  Surv(time = t_tilde, event = 1 - c) ~ x1 + x2,\n  data = simulated,\n  dist = \"exponential\"\n)\nsummary(canned_fit)\n\n\nCall:\nsurvreg(formula = Surv(time = t_tilde, event = 1 - c) ~ x1 + \n    x2, data = simulated, dist = \"exponential\")\n              Value Std. Error     z      p\n(Intercept)  1.0028     0.0411  24.4 &lt;2e-16\nx1          -0.4810     0.0404 -11.9 &lt;2e-16\nx2          -0.4421     0.0415 -10.6 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -1180.2   Loglik(intercept only)= -1300.3\n    Chisq= 240.28 on 2 degrees of freedom, p= 6.7e-53 \nNumber of Newton-Raphson Iterations: 4 \nn= 1000 \n\n\nNote that the canned fit is modeling the mean time to event \\(\\frac{1}{\\lambda}\\) whereas our fit modeled the rate of events \\(\\lambda\\). Because \\(\\beta\\) coefficients are on the scale of \\(\\log(\\lamda)\\), the canned fit estimates are the negative of our DIY estimates."
  },
  {
    "objectID": "exponential.html#simulate-predictions",
    "href": "exponential.html#simulate-predictions",
    "title": "Exponential",
    "section": "Simulate predictions",
    "text": "Simulate predictions\nWe can simulate predicted the hazard and survival probabilities. Suppose we are interested in the prediction at \\((X_1,X_2) = (1,1)\\). Define these data to predict,\n\nto_predict &lt;- tibble(x1 = 0, x2 = 1)\n\ncreate an \\(X\\) matrix at those values\n\nX_new &lt;- model.matrix(~ x1 + x2, data = to_predict)\n\nand make predictions from that \\(X\\) matrix.\n\nlambda_hat &lt;- exp(X_new %*% beta_hat) |&gt; print()\n\n       [,1]\n1 0.5706929\n\n\nWe may not just want the hazard: perhaps we want the probability of surviving past time \\(t = 2\\). Recall that pexp is the CDF, and with the option lower.tail = FALSE it is the survival function.\n\nsurvival_hat &lt;- pexp(2, rate = lambda_hat, lower.tail = FALSE)\n\nTo get a standard error on the predictions, we can use simulation. We know that \\(\\hat{\\vec\\beta}\\) is asymptotically multivariate normal. We can simulate many coefficient vectors \\(\\vec\\beta^*\\) from that distribution,\n\nbeta_star &lt;- mvtnorm::rmvnorm(n = 1000, mean = beta_hat, sigma = beta_hat_vcov)\n\nand we can generate a predicted hazard from each simulated value,\n\nlambda_star &lt;- exp(X_new %*% t(beta_star))\n\nand a predicted survival probability\n\nsurvival_star &lt;- pexp(2, rate = lambda_star, lower.tail = FALSE)\n\nThe standard error is the standard deviation of these simulated draws.\n\nse_survial_hat &lt;- sd(survival_star)"
  },
  {
    "objectID": "exponential.html#canned-comparison",
    "href": "exponential.html#canned-comparison",
    "title": "Exponential",
    "section": "Canned comparison",
    "text": "Canned comparison\nWe can likewise predict with the canned version of the model,\n\ncanned_linear_prediction &lt;- predict(\n  canned_fit, \n  type = \"linear\", \n  newdata = to_predict\n)\n\nRecall that this package has modeled \\(\\vec{X}_i\\beta = \\log(\\frac{1}{\\lambda_i}\\). Thus we need to convert back to \\(\\lambda\\).\n\ncanned_lambda_hat &lt;- 1 / exp(canned_linear_prediction)\n\nNote that they are approximately the same!\n\ncbind(\n  diy = lambda_hat[1,1],\n  canned = canned_lambda_hat\n)\n\n        diy    canned\n1 0.5706929 0.5708173\n\n\nAdvantages of the DIY coding yourself include\n\nyou know exactly how the model worked (e.g., modeling \\(\\lambda\\) vs \\(1 / \\lambda\\))\nyou know how to get standard errors for any quantity of interest\nyou can generalize to models that are not canned"
  },
  {
    "objectID": "exponential.html#univariate-likelihood-in-math",
    "href": "exponential.html#univariate-likelihood-in-math",
    "title": "Exponential",
    "section": "Univariate likelihood in math",
    "text": "Univariate likelihood in math\nLet \\(f(t,\\lambda)\\) be the PDF of the exponential distribution. Let \\(F(t,\\lambda)\\) be the CDF. The likelihood is the probability of observing the data if the parameter takes the value \\(\\lambda\\). The observed data either tells us:\n\nan event occurred at time \\(t\\) (uncensored)\n\noccurs with probability density \\(f(t\\mid\\lambda)\\)\n\nan event occurred at time greater than \\(t\\) (censoring)\n\noccurs with probability \\(1 - F(t\\mid\\lambda)\\)\n\n\nTranslating to math, the likelihood for a given observation is \\[\n\\underbrace{\\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}}_{\\text{PDF at }\\tilde{t}_i\\text{ if uncensored}}\\quad\\times\\quad \\underbrace{\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}}_{\\text{Survival past }\\tilde{t}_i\\text{ if censored}}\n\\]\nWe can put these together into a likelihood function for the vector of independent observations, \\[\nL(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\prod_i \\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}\n\\] and take the log to get the log likelihood. \\[\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda)]\\right)\n\\]"
  },
  {
    "objectID": "exponential.html#univariate-likelihood-in-code",
    "href": "exponential.html#univariate-likelihood-in-code",
    "title": "Exponential",
    "section": "Univariate likelihood in code",
    "text": "Univariate likelihood in code\nWrite the log likelihood as a function in R.\n\nlog_likelihood &lt;- function(log_lambda, data) {\n  data |&gt;\n    mutate(\n      likelihood_i = case_when(\n        c == 0 ~ dexp(t_tilde, rate = exp(log_lambda)),\n        c == 1 ~ pexp(t_tilde, rate = exp(log_lambda), lower.tail = FALSE)\n      )\n    ) |&gt;\n    summarize(\n      log_likelihood = sum(log(likelihood_i))\n    ) |&gt;\n    pull(log_likelihood)\n}"
  },
  {
    "objectID": "exponential.html#one-parameter-likelihood-in-math",
    "href": "exponential.html#one-parameter-likelihood-in-math",
    "title": "Exponential",
    "section": "One-parameter likelihood in math",
    "text": "One-parameter likelihood in math\nLet \\(f(t,\\lambda)\\) be the PDF of the exponential distribution. Let \\(F(t,\\lambda)\\) be the CDF. The likelihood is the probability of observing the data if the parameter takes the value \\(\\lambda\\). The observed data either tells us:\n\nan event occurred at time \\(t\\) (uncensored)\n\noccurs with probability density \\(f(t\\mid\\lambda)\\)\n\nan event occurred at time greater than \\(t\\) (censoring)\n\noccurs with probability \\(1 - F(t\\mid\\lambda)\\)\n\n\nTranslating to math, the likelihood for a given observation is \\[\n\\underbrace{\\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}}_{\\text{PDF at }\\tilde{t}_i\\text{ if uncensored}}\\quad\\times\\quad \\underbrace{\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}}_{\\text{Survival past }\\tilde{t}_i\\text{ if censored}}\n\\]\nWe can put these together into a likelihood function for the vector of independent observations, \\[\nL(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\prod_i \\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}\n\\] and take the log to get the log likelihood. \\[\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda)]\\right)\n\\]"
  },
  {
    "objectID": "exponential.html#one-parameter-likelihood-in-code",
    "href": "exponential.html#one-parameter-likelihood-in-code",
    "title": "Exponential",
    "section": "One-parameter likelihood in code",
    "text": "One-parameter likelihood in code\nWrite the log likelihood as a function in R.\n\nlog_likelihood &lt;- function(log_lambda, data) {\n  data |&gt;\n    mutate(\n      likelihood_i = case_when(\n        c == 0 ~ dexp(t_tilde, rate = exp(log_lambda)),\n        c == 1 ~ pexp(t_tilde, rate = exp(log_lambda), lower.tail = FALSE)\n      )\n    ) |&gt;\n    summarize(\n      log_likelihood = sum(log(likelihood_i))\n    ) |&gt;\n    pull(log_likelihood)\n}"
  },
  {
    "objectID": "exponential.html#vector-parameter-likelihood-in-math",
    "href": "exponential.html#vector-parameter-likelihood-in-math",
    "title": "Exponential",
    "section": "Vector-parameter likelihood in math",
    "text": "Vector-parameter likelihood in math\nThe likelihood has not changed much from the case without predictors. The \\(\\lambda\\) terms become \\(\\lambda_i\\), \\[\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\vec\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda_i)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda_i)]\\right)\n\\]\nand when coding this we will use the assumption that \\(\\lambda_i = \\text{exp}(\\vec{X}_i\\vec\\beta)\\)."
  },
  {
    "objectID": "exponential.html#vector-parameter-likelihood-in-code",
    "href": "exponential.html#vector-parameter-likelihood-in-code",
    "title": "Exponential",
    "section": "Vector-parameter likelihood in code",
    "text": "Vector-parameter likelihood in code\nThe log likelihood takes parameters and data and returns a likelihood value. When coding this, it is helpful that the dexp and pexp functions are the PDF and CDF of the Exponential distribution with rate parameter equal to \\(\\lambda\\).\n\nlog_likelihood &lt;- function(parameters, data, formula) {\n  \n  # Parameters are coefficients\n  beta &lt;- parameters\n  \n  # Get the X matrix\n  X &lt;- model.matrix(formula, data = data)\n  \n  # Calculate lambda values at that parameter vector\n  lambda &lt;- exp(X %*% beta)\n  \n  # Calculate the likelihood\n  data |&gt;\n    # Create a column with the lambda values for each case\n    mutate(lambda = lambda) |&gt;\n    summarize(\n      # Use the formula from above, translated to code\n      log_likelihood = sum(\n        (1 - c) * log(dexp(t_tilde, rate = lambda)) + \n          c * log(1 - pexp(t_tilde, rate = lambda))\n      )\n    ) |&gt;\n    # Pull the log likelihood value to return\n    pull(log_likelihood)\n}"
  }
]