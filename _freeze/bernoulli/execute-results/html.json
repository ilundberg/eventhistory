{
  "hash": "6259c67a9e1dc74cfa178e2389e41430",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bernoulli\"\n---\n\nLet $y_1,\\dots,y_n \\stackrel{\\text{iid}}{\\sim} \\text{Bernoulli}(\\pi)$ be $n$ independent flips of a weighted coin with probability $\\pi$ of heads. In basic statistics, you already learned a common estimator $\\hat\\pi = \\frac{1}{n}\\sum_i y_i$. You also learned that this estimator is asymptotically Normally distributed, with estimated variance $\\hat{\\text{V}}(\\hat\\pi) = \\frac{\\hat\\pi(1 - \\hat\\pi)}{n}$.\n\nThis page will derive those same properties via Maximum Likelihood Estimation.\n\n## Derivations in math\n\n### Without predictors\n\nThe likelihood function is the probability of the data given the parameter $\\pi$.\n\n$$\\begin{aligned}\n        L(\\pi\\mid \\vec{y}) &= \\text{P}(\\vec{Y} = \\vec{y}\\mid \\pi) &\\text{by definition of likelihood} \\\\\n        &= \\prod_{i=1}^n \\text{P}(Y_i = y_i) &\\text{by independence} \\\\\n        &= \\prod_{i=1}^n \\pi^{y_i}(1-\\pi)^{1-y_i} &\\text{by Bernoulli probability mass function}\n\\end{aligned}$$\n\nThe log likelihood is often easier to optimize. Taking the log,\n\n$$\\begin{aligned}\n\\ell(\\pi\\mid\\vec{y}) &= \\log(L(\\pi\\mid\\vec{y})) \\\\\n        &= \\log\\left(\\prod_i \\pi^{y_i}(1-\\pi)^{1-y_i}\\right) \\\\\n        &= \\sum_i\\log\\left(\\pi^{y_i}(1-\\pi)^{1-y_i}\\right) &\\text{since }\\log(AB) = \\log(A) + \\log(B) \\\\\n        &= \\sum_i\\left(\\log(\\pi^{y_i}) + \\log((1-\\pi)^{1-y_i})\\right) &\\text{since }\\log(AB) = \\log(A) + \\log(B) \\\\\n        &= \\sum_i \\left(y_i\\log(\\pi) + (1 - y_i)\\log(1 - \\pi)\\right) &\\text{since}\\log(A^B)=B\\log(A)\n\\end{aligned}$$\n\nWe want to find the estimate $\\hat\\pi$ that maximizes $L(\\pi\\mid\\vec{y})$. This is at a place where the derivative (slope) is zero. To find it, take the derivative.\n\n$$\n\\begin{aligned}\n        \\frac{\\partial}{\\partial \\pi} \\ell(\\pi) \n        &= \\frac{\\sum_i y_i}{\\pi} - \\frac{n - \\sum_i y_i}{1 - \\pi}\n\\end{aligned}\n$$\nSet equal to zero and solve. Let $\\tilde\\pi$ be a point where the derivative equals zero.\n\n$$\n\\begin{aligned}\n        0 &= \\frac{\\partial}{\\partial \\pi} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\tilde{\\pi}} \\\\\n        0 &= \\frac{\\sum_i y_i}{\\tilde\\pi} - \\frac{n - \\sum_i y_i}{1 - \\tilde\\pi} \\\\\n        \\tilde\\pi &= \\frac{1}{n} \\sum_i y_i\n\\end{aligned}\n$$\n\nThe likelihood function is flat at the point where $\\tilde\\pi$ equals the sample mean of $Y$. Next, check the second derivative to see if this is a maximum.\n\n$$\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y}) &= -\\frac{\\sum_i y_i}{\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\pi)^2} \\\\\n&= -\\frac{\\text{positive}}{\\text{positive}} - \\frac{\\text{positive}}{\\text{positive}} \\\\\n        & < 0\n\\end{aligned}$$\n\nSo it is a maximum! The second derivative of the log likelihood at the value that maximizes the function is called the **Hessian**. It is useful for deriving the standard error. Next we derive the Hessian.\n$$\\begin{aligned}\nH(\\pi) &= \\frac{\\partial^2}{\\partial \\pi^2} \\ell(\\pi\\mid \\vec{y})\\rvert_{\\pi = \\hat\\pi_\\text{MLE}} &\\text{definition of Hessian} \\\\\n&= -\\frac{\\sum_i y_i}{\\hat\\pi^2} - \\frac{n - \\sum_i y_i}{(1 - \\hat\\pi)^2} \\\\\n&= -\\frac{n\\hat\\pi}{\\hat\\pi^2} - \\frac{n - n\\hat\\pi}{(1 - \\hat\\pi)^2} &\\text{by replacing }\\sum_i y_i\\text{ with }n\\hat\\pi \\\\\n&= -\\frac{n}{\\hat\\pi}-\\frac{n}{1 - \\hat\\pi} \\\\\n&= -\\frac{n}{\\hat\\pi(1 - \\hat\\pi)}\n\\end{aligned}$$\n\nFrom the statistical theory of Maximum Likelihood Estimation, the asymptotic variance of an MLE estimate is the negative inverse Hessian (also called the **Fisher Information**).\n\n$$\\begin{aligned}\nV(\\hat\\pi) &= -\\left[H(\\hat\\pi)\\right]^{-1} \\\\\n&= \\frac{\\hat\\pi(1 - \\hat\\pi)}{n}\n\\end{aligned}$$\n\nwhich is a formulat that may be familiar! Because of the Central Limit Theorem, $\\hat\\pi$ is asymptotically Normally distributed with this mean and variance. Thus we can construct a 95\\% confidence interval by a Normal approximation.\n\n$$\\begin{aligned}\n\\hat\\pi_\\text{Lower} &= \\hat\\pi - \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}} \\\\\n\\hat\\pi_\\text{Upper} &= \\hat\\pi + \\Phi^{-1}(.975) \\sqrt{\\frac{\\hat\\pi(1 - \\hat\\pi)}{n}}\n\\end{aligned}$$\n\nRecap: The exercise used the theory of Maximum Likelihood Estimation to produce familiar formulas:\n\n* estimator of a proportion is the sample mean\n* variance of that estimated proportion\n* confidence intervals by asymptotic Normality\n\n### With predictors: Logistic regression\n\nWhat if every person $i$ flips their own coin with probability $\\pi_i$, as in logistic regression? Suppose for simplicity that each person has covariates $\\vec{x}_i$ and the probability follows the functional form of logistic regression.\n\n$$\n\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\vec{X}_i\\vec\\beta \n$$\n\nWe will refer to this function as the **logit function** and will use it and its inverse.\n\n$$\n\\begin{align}\n&\\text{Logit function:} & \\text{logit}(x) &= \\log\\left(\\frac{x}{1 - x}\\right) \\\\\n&\\text{Inverse logit function:} & \\text{logit}^{-1}(x) &= \\frac{1}{1 + e^{-x}} \\\\\n\\end{align}\n$$\n\nThese functions are useful because in logistic regression $\\text{logit}(\\pi_i) = \\vec{X}_i\\vec\\beta$ and $\\text{logit}^{-1}(\\vec{X}_i\\vec\\beta) = \\pi$. This is helpful because:\n\n* any likelihood in terms of $\\pi_i$ is equivalently a likelihood in terms of $\\vec\\beta$\n* these functions are available in R: `qlogis` is logit and `plogis` is the inverse logit\n\nMaximum likelihood for logistic regression becomes the task: what parameter vector $\\vec\\beta$ produces a probability vector $\\vec\\pi$ under which the data $\\vec{y}$ are most likely?\n\nAppealing to the marginal case above, the log likelihood is\n\n$$\\begin{aligned}\n\\ell(\\vec\\pi\\mid\\vec{y}) &= \\sum_i\\left(y_i\\log\\pi_i + (1 - y_i)\\log(1 - \\pi_i)\\right)\n\\end{aligned}$$\n\nUsing the inverse logit function, we express this in terms of $\\vec\\beta$.\n$$\\begin{aligned}\n\\ell(\\vec\\beta\\mid\\vec{y}) \n&= \\sum_i\\left(y_i\\log(\\text{logit}^{-1}(\\vec{X}_i\\vec\\beta)) + (1 - y_i)\\log(1 - \\text{logit}^{-1}(\\vec{X}_i\\vec\\beta))\\right)\n\\end{aligned}$$\nRather than solve this analytically, we will work with it by numerically optimizing the log likelihood.\n\n## Numerical optimization\n\nHere we will numerically optimize the log likelihood.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mvtnorm)\n```\n:::\n\n\n### Without predictors\n\nWe first illustrate for the marginal case: estimating a constant $\\pi$ that applies to all cases with no predictors. We know from math that the MLE is the sample mean, $\\hat\\pi_\\text{MLE} = \\frac{1}{n}\\sum_i y_i$. But what if we only knew the likelihood function? We could optimize numerically, as we illustrate with simulated data with $\\pi = 0.4$.\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- rbinom(n = 1000, size = 1, prob = .4)\n```\n:::\n\n\nIn the marginal case, we saw above that the log likelihood is \n$$\n\\ell(\\pi\\mid\\vec{y}) = \\sum_i \\left(y_i\\log(\\pi) + (1 - y_i)\\log(1 - \\pi)\\right)\n$$\n\nWe can code this in a function.\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n```\n:::\n\n\nThen we can use `optimize` to find the maximum of the function.\n\n::: {.cell}\n\n```{.r .cell-code}\noptimize.out <- optimize(\n  log_likelihood,  # function to optimize\n  lower = 0,       # lower limit of pi candidates\n  upper = 1,       # upper limit of pi candidates\n  maximum = TRUE,  # search for maximum\n  tol = .01,       # get within tol of truth\n  y = y            # other argument to log_likelihood\n)\n```\n:::\n\n\nOur estimate is stored in the `maximum` list item of the resulting object.\n\n::: {.cell}\n\n```{.r .cell-code}\noptimize.out$maximum\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4333952\n```\n\n\n:::\n:::\n\nWe can note that this is approximately equal to the analytical result that we know is the maximum: the sample mean. Thus, `optimize` computationally re-created a result we knew from math.\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.433\n```\n\n\n:::\n:::\n\n\n### With predictors\n\nNumerical optimization is often most helpful for generalized linear models that include predictors. For practice, the code below generates data with numeric predictors `x1` and `x2` and binary outcome `y`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- tibble(id = 1:100) |>\n  mutate(\n    x1 = rnorm(n()),\n    x2 = rnorm(n()),\n    pi = plogis(x1 + x2),\n    y = rbinom(n(), 1, pi)\n  )\n```\n:::\n\n\nThe `optim` function in R carries out numerical optimization. To use it, we first need to write down a function for our log likelihood.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(parameters, data, formula) {\n  # Create the beta vector given the parameters\n  beta <- parameters\n  X <- model.matrix(formula, data = data)\n  y <- data$y\n  log_likelihood <- sum(\n    y * log(plogis(X %*% beta)) + \n      (1 - y) * log(1 - plogis(X %*% beta))\n  )\n  return(log_likelihood)\n}\n```\n:::\n\n\nWe optimize this with a call to the `optim` function.\n\n::: {.cell}\n\n```{.r .cell-code}\noptim.out <- optim(\n  par = c(0,0,0), # initial values of the parameters\n  fn = log_likelihood,\n  control = list(fnscale = -1),\n  hessian = TRUE,   # multiplies by -1 to find maximum instead of minimum\n  # Other arguments to be passed to log_likelihood\n  data = simulated,\n  formula = formula(y ~ x1 + x2)\n)\n```\n:::\n\n\nWe can extract the coefficient estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- optim.out$par |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.004142605  0.867452526  0.921018502\n```\n\n\n:::\n:::\n\n\nWe can extract the Hessian.\n\n::: {.cell}\n\n```{.r .cell-code}\nhessian <- optim.out$hessian\n```\n:::\n\n\nWe can solve for the variance-covariance matrix $\\hat{\\text{V}}(\\hat{\\vec\\beta})$: the negative inverse Hessian. In R, the `solve` function finds the inverse of a matrix.\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat_vcov <- -solve(hessian)\n```\n:::\n\n\nThis can give us our coefficients and standard errors (the square root of the diagonal of the variance covariance matrix).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  variable = c(\"Intercept\",\"x1\",\"x2\"),\n  beta = beta_hat,\n  se = sqrt(diag(beta_hat_vcov))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  variable      beta    se\n  <chr>        <dbl> <dbl>\n1 Intercept -0.00414 0.234\n2 x1         0.867   0.257\n3 x2         0.921   0.269\n```\n\n\n:::\n:::\n\n\nWe can compare that to the output from logistic regression.\n\n::: {.cell}\n\n```{.r .cell-code}\ncanned_fit <- glm(y ~ x1 + x2, data = simulated, family = binomial)\nsummary(canned_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial, data = simulated)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.00422    0.23426  -0.018 0.985629    \nx1           0.86761    0.25675   3.379 0.000727 ***\nx2           0.92115    0.26944   3.419 0.000629 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 137.99  on 99  degrees of freedom\nResidual deviance: 109.94  on 97  degrees of freedom\nAIC: 115.94\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n#### Simulating predicted probabilities\n\nWe can simulate predicted probabilities. Suppose we are interested in the prediction at $(X_1,X_2) = (1,1)$. Define these data to predict,\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(x1 = 0, x2 = 1)\n```\n:::\n\ncreate an $X$ matrix at those values\n\n::: {.cell}\n\n```{.r .cell-code}\nX_new <- model.matrix(~ x1 + x2, data = to_predict)\n```\n:::\n\nand make predictions from that $X$ matrix.\n\n::: {.cell}\n\n```{.r .cell-code}\npi_hat <- plogis(X_new %*% beta_hat) |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]\n1 0.7144051\n```\n\n\n:::\n:::\n\n\nTo get a standard error on the predictions, we can use simulation. We know that $\\hat{\\vec\\beta}$ is asymptotically multivariate normal. We can simulate many coefficient vectors $\\vec\\beta^*$ from that distribution,\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_star <- mvtnorm::rmvnorm(n = 1000, mean = beta_hat, sigma = beta_hat_vcov)\n```\n:::\n\nand we can generate a predicted probability from each simulated value.\n\n::: {.cell}\n\n```{.r .cell-code}\npi_star <- plogis(X_new %*% t(beta_star))\n```\n:::\n\nThe standard error is the standard deviation of these simulated draws.\n\n::: {.cell}\n\n```{.r .cell-code}\nse_pi_hat <- sd(pi_star)\n```\n:::\n\n\nWe can likewise predict with the canned version of the model,\n\n::: {.cell}\n\n```{.r .cell-code}\ncanned_prediction <- predict(canned_fit, type = \"response\", newdata = to_predict, se = T)\n```\n:::\n\nand compare to see that they are approximately the same.\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(method = c(\"diy\",\"canned\")) |>\n  mutate(\n    pi_hat = c(pi_hat, canned_prediction$fit),\n    se = c(se_pi_hat, canned_prediction$se.fit)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  method pi_hat     se\n  <chr>   <dbl>  <dbl>\n1 diy     0.714 0.0641\n2 canned  0.714 0.0678\n```\n\n\n:::\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}