{
  "hash": "ee15bdaadef0f83b31fa1a5815f7713e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unmeasured Heterogeneity\"\nformat:\n  html:\n    toc: true\n    toc-expansion: true\n    fig-height: 3\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Simulated illustration\n\nSuppose there are two population subgroups: the frail (with greater risk of death) and the robust (with lower risk of death). The code below simulates data from this hypothetical scenario (click the button to see the code).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(survival)\nsimulated <- tibble(id = 1:1e3) |>\n  mutate(\n    # Make equal-sized subgroups that are frail and robust\n    subgroup = rep(c(\"frail\",\"robust\"), n() / 2),\n    # Define exponential rate parameter in each subgroup\n    rate = case_when(\n      subgroup == \"frail\" ~ 10,\n      subgroup == \"robust\" ~ 1\n    ),\n    # Generate an Exponential outcome\n    t = rexp(n(), rate = rate),\n    # For simplicity, assume no censoring\n    event = 1\n  )\n```\n:::\n\n\nBelow is a visualization of the time until death in each subgroup of this simulation. The **frail subgroup** dies in a short time, while the **robust subgroup** sometimes lives much longer.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Data you observe\n\nNow imagine you see these data, but you do not know which people are `frail` and which are `robust`. You just see the mixture distribution\n\n::: {.cell}\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Exponential model\n\nWithout knowing about the frailty issue, you might estimate an Exponential survival model with no predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpo_model <- survreg(\n  Surv(t, event) ~ 1,\n  data = simulated,\n  dist = \"exponential\"\n)\n```\n:::\n\n\nYou might then visualize the estimated hazard function and survival function. I wrote the [`viz_survreg()`](assets/viz_survreg.R) function to make it easier to visualize these curves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://ilundberg.github.io/eventhistory/viz_survreg.R\")\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpo_model |> viz_survreg()\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nTo know if this is a good model, you might compare the survival curve to a Kaplan-Meier estimate. I wrote the [`km_compare()`](assets/km_compare.R) function to make it easier to visualize these curves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://ilundberg.github.io/eventhistory/km_compare.R\")\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpo_model |> km_compare()\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe comparison suggests something is wrong: the model overestimates survival at early time points, then underestimates survival at late time points. If you did not know the data generating process, you might wonder if this could arise because of a non-constant hazard function.\n\n### Weibull model\n\nKnowing that a Weibull can model a hazard that changes with time, you might then proceed to a Weibull model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweibull_model <- survreg(\n  Surv(t, event) ~ 1,\n  data = simulated,\n  dist = \"weibull\"\n)\n```\n:::\n\n\nPredictions from this model show a hazard that starts high and decreases with time,\n\n::: {.cell}\n\n```{.r .cell-code}\nweibull_model |> viz_survreg()\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nand in fact you can reject the null of the Exponential model (see `summary(weibull_model)` and note that the p-value on `Log(scale) = 0` is very close to 0). You might also be encouraged by seeing that the predicted survival function closely matches Kaplan-Meier.\n\n::: {.cell}\n\n```{.r .cell-code}\nweibull_model |> km_compare()\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### What has happened?\n\nThe illustration above seems paradoxical.\n\n- In the data generating process, each person's survival time was generated by an Exponential. For each person, the hazard was constant over time.\n- In the population average, the hazard function appears to decrease over time.\n\nWhat has happened here is a consequence **unmeasured heterogeneity**. Some of the people are robust, and others are frail. Because the frail die first, the proportion of survivors who are robust rises with time.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThe consequence of this shifting population composition is the appearance of a decreasing hazard function. When `frail` and `robust` are unmeasured, a survival model fitted to the full population will estimate one hazard that decreases over time.\n\nHow much should we worry about unmeasured heterogeneity? On one hand, all properties of the model rely on the assumption that outcomes are actually generated by the model that is selected. On the other hand, a model that only approximates the truth may still yield good population-average or subgroup survival curves even if it does not correspond to the hazard of any individual person.\n\n## Modeling unmeasured heterogeneity\n\nIf you knew the form of the unmodeled heterogeneity, you could define a model that incorporates this unmodeled heterogeneity. In certain circumstances, a correctly-specified model can overcome latent heterogeneity.\n\n### Define a model\n\nFor example, suppose we knew that `simulated` is actually a mixture of two exponential distributions. We could define a data generating process with this mixture.\n\n$$\n\\begin{aligned}\nU &\\sim\\text{Bernoulli}(\\text{probability} = \\pi) \\qquad &&\\text{unmeasured frailty} \\\\\nT &\\sim \\text{Exponential}(\\text{rate} = \\lambda_1^U\\lambda_2^{1-U}) &&\\text{survival time}\n\\end{aligned}\n$$\n\nwhere the survival time is generated with rate $\\lambda_1$ when $U = 1$ and $\\lambda_2$ when $U = 0$.\n\nWe can write down the probability density function of the observed data $t_1,\\dots,t_n$.\n\n$$\n\\begin{aligned}\nf(t) &= f(T = t\\mid U = 1)\\text{P}(U = 1) + f(T = t\\mid U = 0)\\text{P}(U = 0) \\\\\n&= \\underbrace{f(t\\mid\\lambda = \\lambda_1)}_\\text{Exponential PDF}\\pi + \\underbrace{f(t\\mid\\lambda = \\lambda_2)}_\\text{Exponential PDF}(1 - \\pi) \\\\\n\\end{aligned}\n$$\n\n### Code the likelihood\n\nWe can write this log likelihood function in code where `parameters` will be $\\log(\\lambda_1)$, $\\log(\\lambda_2)$, and $\\text{logit}(\\pi)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(parameters, data) {\n  lambda_1 <- exp(parameters[1])\n  lambda_2 <- exp(parameters[2])\n  pi <- plogis(parameters[3])\n  log_likelihood <- data |>\n    mutate(\n      likelihood_i = dexp(t, rate = lambda_1) * pi + \n          dexp(t, rate = lambda_2) * (1 - pi)\n    ) |>\n    summarize(log_likelihood = sum(log(likelihood_i))) |>\n    pull(log_likelihood)\n  return(log_likelihood)\n}\n```\n:::\n\n\n### Optimize\n\nWe can optimize this log likelihood using `optim`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim.out <- optim(\n  par = c(0,0,0),\n  fn = log_likelihood,\n  control = list(fnscale = -1),\n  data = simulated\n)\n```\n:::\n\n\nThis can successfully recover our data generating process! Below are the estimated parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimated_parameters <- tibble(\n  lambda_1 = exp(optim.out$par[1]),\n  lambda_2 = exp(optim.out$par[2]),\n  pi = plogis(optim.out$par[3])\n)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  lambda_1 lambda_2    pi\n     <dbl>    <dbl> <dbl>\n1     1.06     10.2 0.509\n```\n\n\n:::\n:::\n\n\n### Evaluate results\n\nWe can plot a survival curve with these estimated parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncurves <- tibble(t = seq(.01,3,.01)) |>\n  mutate(\n    A = pexp(t, rate = estimated_parameters$lambda_1, lower.tail = FALSE),\n    B = pexp(t, rate = estimated_parameters$lambda_2, lower.tail = FALSE),\n    Pooled = A * estimated_parameters$pi + B * (1 - estimated_parameters$pi)\n  ) |>\n  select(t, A, B, Pooled)\ncurves |>\n  pivot_longer(cols = -t, names_to = \"Latent Subgroup\", values_to = \"Survival\") |>\n  ggplot(aes(x = t, y = Survival, color = `Latent Subgroup`, linetype = `Latent Subgroup`)) +\n  geom_line() +\n  labs(x = \"Time\")\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nWe can also see that the pooled survival curve is incredibly close to the Kaplan-Meier estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsurvfit(Surv(t, event) ~ 1, data = simulated) |>\n  broom::tidy() |>\n  filter(time <= 3) |>\n  select(time, estimate) |>\n  mutate(method = \"Kaplan-Meier\") |>\n  bind_rows(\n    curves |>\n      rename(time = t, estimate = Pooled) |>\n      select(time, estimate) |>\n      mutate(method = \"Latent Mixture of\\nTwo Exponentials\")\n  ) |>\n  ggplot(aes(x = time, y = estimate, color = method, linetype = method)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Survival\", color = \"Method\", linetype = \"Method\")\n```\n\n::: {.cell-output-display}\n![](unmeasured_heterogeneity_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n### Latent heterogeneity in practice\n\nIn practice, it is uncommon to know the parametric form of latent heterogeneity. Thus, models that incorporate latent heterogeneity often rely on assumptions (such as a mixture of two Exponentials) that may themselves be imperfect.\n\nThe more important thing for practice is to recognize that latent heterogeneity is likely to exist in any survival model. When a hazard declines over time, it is important to recognize that this population-level result may arise from two equivalent sources\n\n- individual-level hazard declines over time, or\n- latent heterogeneity so that the frail die first\n\nData can be equally consistent with both interpretations. Nonetheless, one can still produce good survival curve estimates for the full population or in subgroups without distinguishing the two interpretations above.\n\n",
    "supporting": [
      "unmeasured_heterogeneity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}