{
  "hash": "3176cbfd005c361c942cfe5eb7b088d7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential\"\nformat:\n  html:\n    toc: true\n    toc-expansion: true\n---\n\n\n::: {.cell}\n\n:::\n\n\nAn Exponential survival model is a Generalized Linear Model just like logistic regression (previous page). We will estimate this model by writing down the log likelihood and carrying out numerical optimization with `optim`. As with the previous model, we will recover estimates that match those produced by canned functions.\n\nAs a reminder, the Exponential(1) distribution looks like this:\n\n::: {.cell}\n::: {.cell-output-display}\n![](exponential_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Without predictors\n\nAssume a data generating process of $n$ independent observations.\n\n* $t_1,\\dots,t_n\\sim\\text{Exponential}(\\lambda)$ are event times\n* $\\tilde{t}_1,\\dots,\\tilde{t}_n$ are observation times (either events or censoring)\n* $c_1,\\dots c_n$ indicate whether an observation is censored ($c_i = 1$) or the event occurs ($c_i = 0$)\n\nThe code below will simulate data with $\\lambda = 1$ and censoring at $t = 3$.\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- tibble(id = 1:1000) |>\n  mutate(\n    # Exponential draws\n    t = rexp(n()),\n    # Trial cuts off at time 3\n    c = t > 3,\n    # Observed y is truncated at 3\n    t_tilde = ifelse(t > 3, 3, t)\n  )\n```\n:::\n\n\n## One-parameter likelihood in math\n\nLet $f(t,\\lambda)$ be the PDF of the exponential distribution. Let $F(t,\\lambda)$ be the CDF. The likelihood is the probability of observing the data if the parameter takes the value $\\lambda$. The observed data either tells us:\n\n1) an event occurred at time $t$ (uncensored)\n     * occurs with probability density $f(t\\mid\\lambda)$\n2) an event occurred at time greater than $t$ (censoring)\n     * occurs with probability $1 - F(t\\mid\\lambda)$\n     \nTranslating to math, the likelihood for a given observation is\n$$\n\\underbrace{\\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}}_{\\text{PDF at }\\tilde{t}_i\\text{ if uncensored}}\\quad\\times\\quad \\underbrace{\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}}_{\\text{Survival past }\\tilde{t}_i\\text{ if censored}}\n$$\n\nWe can put these together into a likelihood function for the vector of independent observations,\n$$\nL(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\prod_i \\left(f(\\tilde{t}_i\\mid\\lambda)\\right)^{1-c_i}\\left(1 - F(\\tilde{t}_i\\mid\\lambda)\\right)^{c_i}\n$$\nand take the log to get the log likelihood.\n$$\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda)]\\right)\n$$\n\n## One-parameter likelihood in code\n\nWrite the log likelihood as a function in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(log_lambda, data) {\n  data |>\n    mutate(\n      likelihood_i = case_when(\n        c == 0 ~ dexp(t_tilde, rate = exp(log_lambda)),\n        c == 1 ~ pexp(t_tilde, rate = exp(log_lambda), lower.tail = FALSE)\n      )\n    ) |>\n    summarize(\n      log_likelihood = sum(log(likelihood_i))\n    ) |>\n    pull(log_likelihood)\n}\n```\n:::\n\n\n## One-parameter optimization\n\nUsing `optim`, we can numerically find the value $\\hat\\lambda$ that maximizes the log likelihood function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimize.out <- optimize(\n  log_likelihood,  # function to optimize\n  lower = -1,      # lower limit of log_lambda candidates\n  upper = 1,       # upper limit of log_lambda candidates\n  maximum = TRUE,  # search for maximum\n  tol = .01,       # get within tol of truth\n  data = simulated # other argument to log_likelihood\n)\n```\n:::\n\n\nRemember that we set our parameter to be the log of the rate $\\lambda$. Thus, we need to exponentiate the estimated parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_lambda_hat <- optimize.out$maximum\nlambda_hat <- exp(log_lambda_hat) |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9808973\n```\n\n\n:::\n:::\n\n\n## With predictors\n\nNow consider the setting where $\\lambda_i$ varies across units $i$ according to a Generalized Linear model,\n$$\n\\begin{aligned}\ny_i&\\sim\\text{Exponential}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\vec{X}_i\\vec\\beta\n\\end{aligned}\n$$\n\nThe code below generates data to illustrate with two predictors, $X_1$ and $X_2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- tibble(id = 1:1000) |>\n  mutate(\n    x1 = rnorm(n()),\n    x2 = rnorm(n()),\n    lambda = exp(-1 + .5 * x1 + .5 * x2),\n    t = rexp(n(), rate = lambda),\n    # Create censoring at time 3\n    c = t > 3,\n    t_tilde = ifelse(t > 3, 3, t)\n  ) |>\n  select(x1, x2, c, t_tilde)\n```\n:::\n\n\n## Vector-parameter likelihood in math\n\nThe likelihood has not changed much from the case without predictors. The $\\lambda$ terms become $\\lambda_i$,\n$$\n\\ell(\\vec{\\tilde{t}},\\vec{c}\\mid\\vec\\lambda) = \\sum_i \\left((1-c_i)\\log[f(\\tilde{t}_i\\mid\\lambda_i)] + c_i\\log[1 - F(\\tilde{t}_i\\mid\\lambda_i)]\\right)\n$$\n\nand when coding this we will use the assumption that $\\lambda_i = \\text{exp}(\\vec{X}_i\\vec\\beta)$.\n\n## Vector-parameter likelihood in code\n\nThe log likelihood takes parameters and data and returns a likelihood value. When coding this, it is helpful that the `dexp` and `pexp` functions are the PDF and CDF of the Exponential distribution with `rate` parameter equal to $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(parameters, data, formula) {\n  \n  # Parameters are coefficients\n  beta <- parameters\n  \n  # Get the X matrix\n  X <- model.matrix(formula, data = data)\n  \n  # Calculate lambda values at that parameter vector\n  lambda <- exp(X %*% beta)\n  \n  # Calculate the likelihood\n  data |>\n    # Create a column with the lambda values for each case\n    mutate(lambda = lambda) |>\n    summarize(\n      # Use the formula from above, translated to code\n      log_likelihood = sum(\n        (1 - c) * log(dexp(t_tilde, rate = lambda)) + \n          c * log(1 - pexp(t_tilde, rate = lambda))\n      )\n    ) |>\n    # Pull the log likelihood value to return\n    pull(log_likelihood)\n}\n```\n:::\n\n\n## Vector-parameter optimization\n\nNow optimize with a call to `optim`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim.out <- optim(\n  par = c(0,0,0),                # initial parameter values\n  fn = log_likelihood,           # function to optimize\n  control = list(fnscale = -1),  # find max instead of min\n  hessian = TRUE,                # also return the Hessian\n  data = simulated,              # passed to log_likelihood\n  formula = formula(t_tilde ~ x1 + x2) # passed to log_likelihood\n)\n```\n:::\n\n\nWe can extract the coefficient estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- optim.out$par |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.0179787  0.4640804  0.6061929\n```\n\n\n:::\n:::\n\n\nWe can extract the Hessian.\n\n::: {.cell}\n\n```{.r .cell-code}\nhessian <- optim.out$hessian\n```\n:::\n\n\nWe can solve for the variance-covariance matrix $\\hat{\\text{V}}(\\hat{\\vec\\beta})$: the negative inverse Hessian. In R, the `solve` function finds the inverse of a matrix.\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat_vcov <- -solve(hessian)\n```\n:::\n\n\nThis can give us our coefficients and standard errors (the square root of the diagonal of the variance covariance matrix).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  variable = c(\"Intercept\",\"x1\",\"x2\"),\n  beta = beta_hat,\n  se = sqrt(diag(beta_hat_vcov))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  variable    beta     se\n  <chr>      <dbl>  <dbl>\n1 Intercept -1.02  0.0416\n2 x1         0.464 0.0405\n3 x2         0.606 0.0429\n```\n\n\n:::\n:::\n\n\nWe can compare that to the output from logistic regression.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\ncanned_fit <- survreg(\n  Surv(time = t_tilde, event = 1 - c) ~ x1 + x2,\n  data = simulated,\n  dist = \"exponential\"\n)\nsummary(canned_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsurvreg(formula = Surv(time = t_tilde, event = 1 - c) ~ x1 + \n    x2, data = simulated, dist = \"exponential\")\n              Value Std. Error     z      p\n(Intercept)  1.0181     0.0416  24.5 <2e-16\nx1          -0.4642     0.0405 -11.5 <2e-16\nx2          -0.6062     0.0429 -14.1 <2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -1149.8   Loglik(intercept only)= -1301.9\n\tChisq= 304.19 on 2 degrees of freedom, p= 8.8e-67 \nNumber of Newton-Raphson Iterations: 4 \nn= 1000 \n```\n\n\n:::\n:::\n\n\nNote that the canned fit is modeling the mean time to event $\\frac{1}{\\lambda}$ whereas our fit modeled the rate of events $\\lambda$. Because $\\beta$ coefficients are on the scale of $\\log(\\lamda)$, the canned fit estimates are the negative of our DIY estimates.\n\n## Simulate predictions\n\nWe can simulate predicted the hazard and survival probabilities. Suppose we are interested in the prediction at $(X_1,X_2) = (1,1)$. Define these data to predict,\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(x1 = 0, x2 = 1)\n```\n:::\n\ncreate an $X$ matrix at those values\n\n::: {.cell}\n\n```{.r .cell-code}\nX_new <- model.matrix(~ x1 + x2, data = to_predict)\n```\n:::\n\nand make predictions from that $X$ matrix.\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_hat <- exp(X_new %*% beta_hat) |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]\n1 0.6624662\n```\n\n\n:::\n:::\n\n\nWe may not just want the hazard: perhaps we want the probability of surviving past time $t = 2$. Recall that `pexp` is the CDF, and with the option `lower.tail = FALSE` it is the survival function.\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvival_hat <- pexp(2, rate = lambda_hat, lower.tail = FALSE)\n```\n:::\n\n\nTo get a standard error on the predictions, we can use simulation. We know that $\\hat{\\vec\\beta}$ is asymptotically multivariate normal. We can simulate many coefficient vectors $\\vec\\beta^*$ from that distribution,\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_star <- mvtnorm::rmvnorm(n = 1000, mean = beta_hat, sigma = beta_hat_vcov)\n```\n:::\n\nand we can generate a predicted hazard from each simulated value,\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_star <- exp(X_new %*% t(beta_star))\n```\n:::\n\nand a predicted survival probability\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvival_star <- pexp(2, rate = lambda_star, lower.tail = FALSE)\n```\n:::\n\nThe standard error is the standard deviation of these simulated draws.\n\n::: {.cell}\n\n```{.r .cell-code}\nse_survial_hat <- sd(survival_star)\n```\n:::\n\n\n## Canned comparison\n\nWe can likewise predict with the canned version of the model,\n\n::: {.cell}\n\n```{.r .cell-code}\ncanned_linear_prediction <- predict(\n  canned_fit, \n  type = \"linear\", \n  newdata = to_predict\n)\n```\n:::\n\n\nRecall that this package has modeled $\\vec{X}_i\\beta = \\log(\\frac{1}{\\lambda_i}$. Thus we need to convert back to $\\lambda$.\n\n::: {.cell}\n\n```{.r .cell-code}\ncanned_lambda_hat <- 1 / exp(canned_linear_prediction)\n```\n:::\n\n\nNote that they are approximately the same!\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(\n  diy = lambda_hat[1,1],\n  canned = canned_lambda_hat\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        diy    canned\n1 0.6624662 0.6623683\n```\n\n\n:::\n:::\n\n\nAdvantages of the DIY coding yourself include\n\n* you know exactly how the model worked (e.g., modeling $\\lambda$ vs $1 / \\lambda$)\n* you know how to get standard errors for any quantity of interest\n* you can generalize to models that are not canned\n\n",
    "supporting": [
      "exponential_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}